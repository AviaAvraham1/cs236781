---
title: "Lecture 5: Sequence Models"
header:
    teaser: assets/images/lec5/rnn.png
---

RNN model, input-output sequences relationships, non-sequential input, layered RNN,
backpropagation through time, word embeddings, attention, transformers.

{% include figure image_path="assets/images/lec5/tbptt.png" alt="" caption="" ref="" %}

## Slides

- Part I: RNNs

{% include ppt_embed.html
src="https://onedrive.live.com/embed?cid=449B779EEB6DCD79&amp;resid=449B779EEB6DCD79%21208&amp;authkey=AC2UftXfSCJp_lQ&amp;em=2&amp;wdAr=1.6" %}

- Part II: Attention and Transformers

{% include ppt_embed.html
src="https://onedrive.live.com/embed?cid=449B779EEB6DCD79&amp;resid=449B779EEB6DCD79%21290&amp;authkey=AGPMyuvEmiUGqdQ&amp;em=2&amp;wdAr=1.7777777777777777" %}

## Videos

- Part I

{% include video provider="youtube" id="_mQf44nB4JQ" %}

- Part II: Word embeddings

{% include video provider="youtube" id="rZSnN5LvvkA" %}

## Lecture Notes

Accompanying notes for this lecture can be found [here]({{ site.baseurl }}{% link _lecture_notes/lecture_05.md %}).
